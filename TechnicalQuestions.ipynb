{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# A tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key is valid\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith(\"sk-proj-\") and len(api_key)>10:\n",
    "    print(\"API key is valid\")\n",
    "else:\n",
    "    print(\"API key is invalid. Please chech your .env file\")\n",
    "\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "openai = OpenAI()\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = f\"\"\"You are an expert AI assistant specialized in answering technical questions with clear, detailed, and accurate explanations. \n",
    "Your goal is to provide responses that are:\n",
    "- **Accurate**: Ensure factual correctness in your explanations.\n",
    "- **Concise yet Detailed**: Keep responses structured and to the point while covering important details.\n",
    "- **Beginner-Friendly but Technical**: Adapt explanations based on the complexity of the question.\n",
    "- **Example-Driven**: Whenever possible, provide real-world examples or code snippets to clarify concepts.\n",
    "\n",
    "Respond in a structured format:\n",
    "1️⃣ **Definition**: Start with a brief explanation of the concept.\n",
    "2️⃣ **How it Works**: Explain its functionality or underlying principles.\n",
    "3️⃣ **Use Cases**: Describe where and how it is used.\n",
    "4️⃣ **Example (if applicable)**: Provide a relevant example or code snippet.\n",
    "5️⃣ **Common Mistakes & Best Practices**: Highlight any pitfalls or recommendations.\n",
    "\n",
    "If a question is ambiguous, ask for clarification before answering.\n",
    "If the question is outside of your scope, politely indicate that and suggest possible sources for information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model = MODEL,\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "    stream = True\n",
    ")\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id = True)\n",
    "for chunk in stream:\n",
    "    response +=chunk.choices[0].delta.content or \"\"\n",
    "    response = response.replace(\"``\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id = display_handle.display_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a36d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODELOLL = \"llama3.2\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": MODELOLL,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9351a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Definition**: `yield from` is a Python syntax introduced in version 3.3, which allows you to delegate iteration over a sub-iterator.\n",
       "\n",
       "**How it Works**: In this code snippet, we're using `yield from` to iterate over a list of books and extract the author information from each book. The expression `{book.get(\"author\") for book in books if book.get(\"author\")}` is an iterator that yields values on the fly, without storing them in memory.\n",
       "\n",
       "Here's a step-by-step breakdown:\n",
       "\n",
       "1. `for book in books`: This part iterates over a list of `books`.\n",
       "2. `if book.get(\"author\")`: For each book, this condition checks if the \"author\" key exists and has a non-empty value.\n",
       "3. `{book.get(\"author\") for ...}`: If the condition is met, the dictionary's \"author\" value (or `None` if it doesn't exist) is yielded.\n",
       "\n",
       "**Use Cases**: This syntax is commonly used when working with data structures that are too large to fit into memory or when you need to process a large dataset in chunks. It's particularly useful for:\n",
       "\n",
       "* Reading large files or datasets\n",
       "* Handling infinite iterables (e.g., generator expressions)\n",
       "* Improving memory efficiency\n",
       "\n",
       "**Example**:\n",
       "`python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": None},\n",
       "    {\"title\": \"Book 3\", \"author\": \"Author C\"}\n",
       "]\n",
       "\n",
       "for author in yield from {book.get(\"author\") for book in books if book.get(\"author\")]:\n",
       "    print(author)\n",
       "`\n",
       "Output:\n",
       "`\n",
       "Author A\n",
       "None\n",
       "Author C\n",
       "`\n",
       "\n",
       "**Common Mistakes & Best Practices**: When using `yield from`, ensure that the sub-iterator is properly initialized and that you handle any potential errors. Additionally, be mindful of memory usage when working with large datasets.\n",
       "\n",
       "**Additional Advice**:\n",
       "\n",
       "* If you're familiar with generators, you might recognize this syntax as equivalent to a generator expression.\n",
       "* To iterate over the yielded values instead of using `yield from`, you can use the `.next()` method or a `for` loop.\n",
       "`python\n",
       "author_iterator = {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "while True:\n",
       "    try:\n",
       "        author = next(author_iterator)\n",
       "        print(author)\n",
       "    except StopIteration:\n",
       "        break\n",
       "`\n",
       "This approach can be useful when you need to iterate over the yielded values after they've been generated."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = requests.post(OLLAMA_API, headers=HEADERS, json = payload, stream = True)\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id = True)\n",
    "for chunk in stream.iter_lines(decode_unicode=True):\n",
    "    if chunk.strip():\n",
    "        try:\n",
    "            data = json.loads(chunk)\n",
    "            \n",
    "            response += data[\"message\"][\"content\"]\n",
    "\n",
    "            response = response.replace(\"``\",\"\").replace(\"markdown\", \"\")\n",
    "            update_display(Markdown(response), display_id = display_handle.display_id)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: Skipped malformed JSON chunk: {chunk} - Error: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
